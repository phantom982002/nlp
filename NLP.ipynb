{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Material\n",
    "\n",
    "- https://arxiv.org/pdf/1805.03818.pdf\n",
    "- https://cs.stanford.edu/people/chrismre/papers/dd.pdf\n",
    "- https://cs.stanford.edu/people/chrismre/papers/deepdive_highlight.pdf\n",
    "- https://arxiv.org/pdf/1711.10160.pdf\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd ~/\n",
    "curl -LOJ http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
    "unzip stanford-corenlp-full-2018-10-05.zip\n",
    "pip install stanfordnlp warc3-wet bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from pprint import pprint\n",
    "from stanfordnlp.server import CoreNLPClient\n",
    "\n",
    "environ['CORENLP_HOME'] = f'{environ[\"HOME\"]}/stanford-corenlp-full-2018-10-05/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Parsing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot = ['parse']\n",
    "nlp = CoreNLPClient(\n",
    "    annotators=annot,\n",
    "    timeout=3000,\n",
    "    memory='16G'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Sentence and Print Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prpr(t,v=False):\n",
    "    w = [prpr(c) for c in t.child]\n",
    "    w = [t.value, w] if v else w\n",
    "    return w if t.child else t.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[['I']],\n",
      "  [['went'],\n",
      "   [['to'], [['the'], ['store']]],\n",
      "   [[['to'], [['get'], [['ice'], ['cream']]]]]],\n",
      "  ['.']]]\n"
     ]
    }
   ],
   "source": [
    "nlp.register_properties_key('multiparse', {'parse.kbest': 10})\n",
    "doc = nlp.annotate('I went to the store to get ice cream.', properties_key='multiparse')\n",
    "\n",
    "pprint(prpr(doc.sentence[0].kBestParseTrees[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, gzip, io, warc, bs4, http, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_base = 'https://commoncrawl.s3.amazonaws.com'\n",
    "cc_path = 'crawl-data/CC-MAIN-2019-47'\n",
    "cc_idx = 'crawl-data/CC-MAIN-2019-47/cc-index.paths.gz'\n",
    "cc_url = f'{cc_base}/{cc_path}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List WARC files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(f'{cc_url}/warc.paths.gz')\n",
    "paths = gzip.decompress(res.content).decode('utf8').split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Crawled URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(f'{cc_base}/{cc_idx}')\n",
    "shards = gzip.decompress(res.content).decode('utf8').split('\\n')\n",
    "\n",
    "res = requests.get(f'{cc_base}/{shards[1]}')\n",
    "urls = gzip.decompress(res.content).decode('utf8').split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map crawl records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "url, timestamp, data = urls[0].split(' ', 2)\n",
    "crawled_at = json.loads(data)\n",
    "crawled_at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read records from WARC archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(f'{cc_base}/{paths[0]}')\n",
    "arc = warc.warc.WARCFile(\n",
    "    fileobj=io.BytesIO(gzip.decompress(res.content))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = next(iter(arc))\n",
    "url = record['WARC-Target-URI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = http.client.parse_headers(record.payload)\n",
    "body = bs4.BeautifulSoup(record.payload.read(), 'html.parser')\n",
    "links = [link.get('href') for link in body.find_all('a')]\n",
    "text = body.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bing API\n",
    "\n",
    "Get API key from https://azure.microsoft.com/en-us/services/cognitive-services/bing-web-search-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "BING_API_KEY=open('.bingAPIKey').read().strip()\n",
    "BING_SEARCH_ENDPOINT = 'https://api.cognitive.microsoft.com/bing/v7.0/search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '\"powered by phpbb\"'\n",
    "results = requests.get(\n",
    "    BING_SEARCH_ENDPOINT, \n",
    "    params={'q': query, 'count': 100, 'offset': 0},\n",
    "    headers={'Ocp-Apim-Subscription-Key': BING_API_KEY}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = results.json()\n",
    "pages = res.get('webPages').get('value')\n",
    "sites = [page.get('url') for page in pages]\n",
    "total = res.get('totalEstimatedMatches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit API\n",
    "\n",
    "### Find subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_base = 'https://api.reddit.com'\n",
    "sub_search = 'subreddits/search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'hexadecimal'\n",
    "results = requests.get(\n",
    "    f'{reddit_base}/{sub_search}',\n",
    "    params={'q': query}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.json().get('data').get('children')\n",
    "subreddits = [sub.get('data').get('display_name') for sub in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get new posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = 'scifi'\n",
    "results = requests.get(f'{reddit_base}/r/{subreddit}/top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = results.json().get('data').get('children')\n",
    "\n",
    "transform = lambda post: {\n",
    "    'name': post.get('title'), \n",
    "    'id': post.get('id'),\n",
    "    'subreddit': post.get('subreddit'),\n",
    "    'created': post.get('created_utc'),\n",
    "    'score': post.get('score'),\n",
    "}\n",
    "\n",
    "posts = [transform(post.get('data')) for post in posts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get comment tree\n",
    "\n",
    "Use the series of comment timestamps to [predict popular posts](https://snikolov.wordpress.com/2012/11/14/early-detection-of-twitter-trends/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = requests.get(\n",
    "    f'{reddit_base}/r/{subreddit}/comments/{posts[0].get(\"id\")}'\n",
    ")\n",
    "\n",
    "comments = []\n",
    "queue = results.json()\n",
    "\n",
    "while len(queue) > 0:\n",
    "    current = queue.pop(0)\n",
    "    kind = current.get('kind')\n",
    "    current = current.get('data')\n",
    "    \n",
    "    if kind == 't1':\n",
    "        comments.append({\n",
    "            'created': current.get('created_utc'),\n",
    "            'article': current.get('link_id'),\n",
    "            'name': current.get('name'),\n",
    "            'parent': current.get('parent_id')\n",
    "        })\n",
    "    queue += current.get('children', [])\n",
    "    \n",
    "    if type(current.get('replies') ) == dict:\n",
    "        queue += current.get('replies').get('data', {}).get('children', [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
